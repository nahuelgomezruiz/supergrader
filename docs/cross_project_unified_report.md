# Cross-Project Unified Report

## Overview

The Cross-Project Unified Report feature generates a comprehensive Excel file that consolidates evaluation statistics from all evaluated assignments (hw_linkedlists, proj_gerp, etc.) into a single, easy-to-analyze document. This eliminates the need to open individual files to understand overall evaluation performance.

## What Gets Generated

### ðŸ“„ **Single Excel File**
- **Filename**: `CROSS_PROJECT_UNIFIED_evaluation.xlsx`
- **Location**: Generated in the main output directory alongside individual project reports
- **Content**: 4 comprehensive worksheets with different analysis perspectives

## Worksheets Included

### 1. ðŸ“Š **OVERVIEW_SUMMARY**
**Main dashboard with key statistics for each assignment**

| Project | Assignment | Students Evaluated | Total Rubric Items | Accuracy Rate | Avg Confidence | Checkbox Items | Radio Items |
|---------|------------|-------------------|-------------------|---------------|----------------|----------------|-------------|
| hw_linkedlists | section_01_grading | 15 | 8 | 87.5% | 82.3% | 6 | 2 |
| proj_gerp | section_01_grading | 12 | 12 | 91.2% | 85.7% | 8 | 4 |

**Features:**
- âœ… **Color-coded accuracy**: Green (â‰¥90%), Blue (â‰¥80%), Yellow (â‰¥70%), Red (<70%)
- ðŸ“Š **Totals/Averages row** at the bottom
- ðŸ“ˆ **Comprehensive metrics** for quick performance assessment

### 2. ðŸ” **DETAILED_STATISTICS**
**Per-rubric-item breakdown with granular analysis**

| Project | Assignment | Rubric Item | Type | Points | Matches | False Positives | False Negatives | Accuracy | Avg Confidence |
|---------|------------|-------------|------|--------|---------|----------------|----------------|----------|----------------|
| hw_linkedlists | section_01 | Functionality works correctly... | CHECKBOX | 3.0 | 12 | 2 | 1 | 80.0% | 78.5% |

**Features:**
- ðŸŽ¯ **Individual rubric item performance**
- âš–ï¸ **False positive/negative tracking**
- ðŸ“‹ **Truncated descriptions** for readability (50 chars + "...")
- ðŸŒˆ **Color-coded accuracy** for quick identification of problem areas

### 3. ðŸ“ˆ **PROJECT_COMPARISON**
**High-level comparison between different projects**

| Project | Total Assignments | Total Students | Total Rubric Items | Overall Accuracy | Avg Confidence | Best Assignment | Worst Assignment | Notes |
|---------|------------------|----------------|-------------------|------------------|----------------|----------------|-----------------|-------|
| hw_linkedlists | 3 | 45 | 24 | 88.3% | 81.2% | section_03_grading | section_01_grading | Good performance; High confidence |

**Features:**
- ðŸ† **Best/worst assignment identification**
- ðŸ“ **Automated performance notes**
- ðŸ“Š **Project-level aggregation**
- ðŸ” **Confidence level assessment**

### 4. ðŸŽ¯ **RUBRIC_TYPE_ANALYSIS**
**Separated analysis for checkbox vs radio button items**

**Checkbox Items:**
| Project | Assignment | Accuracy | Avg Confidence | Total Items | Performance |
|---------|------------|----------|----------------|-------------|-------------|
| proj_gerp | section_01 | 92.1% | 87.3% | 8 | Excellent |

**Radio Items:**
| Project | Assignment | Accuracy | Avg Confidence | Total Items | Performance |
|---------|------------|----------|----------------|-------------|-------------|
| proj_gerp | section_01 | 89.5% | 83.2% | 4 | Good |

**Features:**
- ðŸ”² **Checkbox performance analysis**
- ðŸ”˜ **Radio button performance analysis** 
- ðŸ“Š **Type-specific insights**
- ðŸŽ¯ **Performance categorization**: Excellent (â‰¥90%), Good (â‰¥80%), Fair (â‰¥70%), Poor (<70%)

## How to Access

### ðŸ–¥ï¸ **Via GUI (Recommended)**
1. Open the evaluation GUI: `python evaluation_gui.py`
2. Select assignments you want to evaluate
3. **Important**: Choose **"Concurrent"** processing mode
4. Run the evaluation
5. The unified report will be automatically generated at the end

### ðŸ’» **Via Command Line**
```bash
cd evaluation
python evaluation_dashboard.py --concurrent --num-students 20
```

### ðŸ“ **File Location**
The report will be saved in your output directory as:
```
output/
â”œâ”€â”€ run_20241201_143022/
â”‚   â”œâ”€â”€ hw_linkedlists_UNIFIED_evaluation.xlsx    # Individual project
â”‚   â”œâ”€â”€ proj_gerp_UNIFIED_evaluation.xlsx         # Individual project  
â”‚   â””â”€â”€ CROSS_PROJECT_UNIFIED_evaluation.xlsx     # â† CROSS-PROJECT REPORT
```

## Key Benefits

### ðŸŽ¯ **Consolidated Overview**
- **Single file** contains all assignment statistics
- **No need** to open multiple Excel files
- **Quick identification** of trends and patterns

### ðŸ“Š **Comprehensive Analysis**
- **4 different perspectives** on the same data
- **Color-coded performance** for visual analysis
- **Automatic categorization** and notes

### âš¡ **Efficiency Gains**
- **Instant comparison** between assignments
- **Easy identification** of problematic rubric items
- **Clear performance trends** across projects

### ðŸ” **Detailed Insights**
- **Rubric item level** analysis
- **Type-specific performance** (checkbox vs radio)
- **False positive/negative tracking**

## Color Coding System

### ðŸŽ¨ **Accuracy Performance**
- ðŸŸ¢ **Green (â‰¥90%)**: Excellent performance
- ðŸ”µ **Blue (â‰¥80%)**: Good performance  
- ðŸŸ¡ **Yellow (â‰¥70%)**: Fair performance
- ðŸ”´ **Red (<70%)**: Needs improvement

### ðŸ“ **Performance Categories**
- **Excellent**: â‰¥90% accuracy
- **Good**: 80-89% accuracy
- **Fair**: 70-79% accuracy
- **Poor**: <70% accuracy

## Sample Analysis Workflow

### 1. ðŸ“‹ **Start with OVERVIEW_SUMMARY**
- Identify which assignments have lowest accuracy
- Check if any projects consistently underperform
- Look for patterns in confidence scores

### 2. ðŸ” **Drill down to DETAILED_STATISTICS**
- Find specific rubric items causing problems
- Identify patterns in false positives/negatives
- Focus improvement efforts on lowest-performing items

### 3. ðŸ“ˆ **Review PROJECT_COMPARISON**
- Compare performance across different course modules
- Identify best practices from high-performing assignments
- Plan targeted improvements for underperforming areas

### 4. ðŸŽ¯ **Analyze RUBRIC_TYPE_ANALYSIS**
- Compare AI performance on checkbox vs radio items
- Adjust rubric design based on type-specific performance
- Optimize question formats for better AI accuracy

## Technical Implementation

### ðŸ—ï¸ **Architecture**
- **Class**: `CrossProjectUnifiedReportGenerator`
- **Location**: `evaluation/evaluation_dashboard.py`
- **Dependencies**: `openpyxl`, `pathlib`, existing evaluation infrastructure

### ðŸ”„ **Data Flow**
1. Individual evaluations completed for each project
2. Cross-project data aggregated from all evaluation results
3. Four worksheet analysis performed
4. Excel file generated with formatted output

### ðŸ“Š **Data Sources**
- **Human grades**: From CSV files in `eval-data/*/grades/`
- **AI evaluations**: From backend API responses
- **Rubric items**: From generated rubric text files
- **Student data**: From evaluation processing

## Availability

### âœ… **Available In:**
- **Concurrent processing mode** (GUI and command line)
- **All evaluation runs** with multiple projects

### âŒ **Not Available In:**
- **Sequential processing mode** (legacy mode)
- **Single project evaluations**

### ðŸ“ **Note for Sequential Mode Users**
When using sequential processing, the GUI will display:
```
ðŸ“ Note: Cross-project unified reports are only generated in concurrent processing mode.
   To get comprehensive statistics across all assignments, please use concurrent mode.
```

## Performance Considerations

### ðŸ“ˆ **Generation Time**
- **Minimal overhead**: Generated after all individual reports
- **Memory efficient**: Processes aggregated data, not raw submissions
- **Fast Excel writing**: Uses optimized openpyxl operations

### ðŸ’¾ **File Size**
- **Typical size**: 100-500KB depending on number of assignments
- **Compressed format**: Excel's native compression
- **Optimized layouts**: Efficient data representation

## Troubleshooting

### ðŸ”§ **Common Issues**

**Report not generated:**
- âœ… Ensure you're using **concurrent processing mode**
- âœ… Check that **multiple projects** were evaluated
- âœ… Verify output directory permissions

**Missing data in report:**
- âœ… Confirm all individual project evaluations completed successfully
- âœ… Check for errors in evaluation logs
- âœ… Ensure rubric files are properly formatted

**Excel file won't open:**
- âœ… Check file permissions in output directory
- âœ… Ensure Excel/compatible software is available
- âœ… Verify file wasn't corrupted during generation

### ðŸ“‹ **Verification Steps**
1. **Check console output** for report generation confirmation
2. **Verify file exists** in output directory
3. **Open file** and check all 4 worksheets are present
4. **Validate data** matches individual project reports

## Future Enhancements

### ðŸš€ **Potential Additions**
- **Time series analysis** for tracking improvement over time
- **Student-level aggregation** across assignments
- **Automated recommendations** based on performance patterns
- **Export to other formats** (PDF, CSV) 
- **Interactive dashboards** with filtering capabilities

### ðŸ”§ **Configuration Options**
- **Customizable thresholds** for performance categories
- **Selectable worksheets** to include/exclude
- **Custom color schemes** for different use cases
- **Additional statistical measures** (standard deviation, etc.)

## Summary

The Cross-Project Unified Report provides a powerful, comprehensive view of AI evaluation performance across all assignments. With four specialized worksheets, color-coded performance indicators, and detailed statistical analysis, it enables instructors and administrators to:

- **Quickly assess** overall system performance
- **Identify areas** needing improvement  
- **Compare performance** across different assignments
- **Make data-driven decisions** about rubric design and AI tuning

This feature transforms individual evaluation results into actionable insights for continuous improvement of the automated grading system! ðŸŽ¯ 